{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Long/Short Methodologies: Absolute and Relative\n",
    "\n",
    "This chapter introduces the relative series methodology\n",
    "\n",
    "1. Build a dataframe of S&P 500 constituents\n",
    "2. Download historical prices with yfinance library\n",
    "3. Bullish/Bearish regime in absolute/relative series\n",
    "4. Relative  & Utilities Function\n",
    "5. Softbank in absolute JPY & relative to Nasdaq 100 in USD\n",
    "6. GEMS Stocks relative to S&P 500\n",
    "7. Sector Average\n",
    "8. Cyclicals v. defensives\n",
    "9. Cross-market comparison\n",
    "10. Disney MCU rise and fall\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import datetime \n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build a dataframe of S&P 500 constituents\n",
    "\n",
    "1. Pandas read_html is an efficient way to convert tables from the internet into dataframe. \n",
    "2. Webpages often contain multiple tables, so make sure you index the correct table. Here we reference the first table on the page [0]\n",
    "3. Next, we rename columns. Different webpages have different names for the same fields: symbol v. ticker, GICS sector v. sector etc. We make a habit of harmonising names to facilitate processing\n",
    "4. We will be downloading historical prices from Yahoo. So, we need to convert tickers into a format compatible with the yfinance library: .str.replace('.', '-', regex =False)\n",
    "5. We sort tickers by sector and ticker in ascending order\n",
    "Tip: Tables and webpages get updated regularly so make sure to check the shape of the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "df_SP500 = df_SP500.rename(columns={'Symbol':'ticker', 'Security': 'name', 'GICS Sector':'sector','GICS Sub-Industry':'sub-industry'})\n",
    "df_SP500['ticker'] = df_SP500['ticker'].str.replace('.', '-', regex =False)\n",
    "df_SP500 = df_SP500.sort_values(by = ['sector','name']).set_index('ticker')\n",
    "\n",
    "bm_ticker = '^GSPC'\n",
    "tickers_list = [bm_ticker] + list(df_SP500.index)[:]\n",
    "df_SP500.groupby('sector').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download historical prices with yfinance library\n",
    "\n",
    "1.\tbatch download \"Close\" prices from yfinance\n",
    "    1.\ttickers_list: make sure all tickers are compatible with the yfinance library \n",
    "    2.\tbatch_size: data is processed in batches. Single download sometimes generate errors\n",
    "    3.\tstart, end: self-explanatory\n",
    "    4.\tshow_batch = True, shows tickers as they are downloaded\n",
    "    5.\tLoop to download ['Close'] prices in batches\n",
    "    6.\tPopulate the px_df by joining each batch and return px_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_px_df(tickers_list,batch_size, start, end,show_batch = False):\n",
    "    px_df = pd.DataFrame()\n",
    "    loop_size = int(len(tickers_list) // batch_size) + 2\n",
    "    for t in range(1,loop_size): # Batch download\n",
    "        try:\n",
    "            m = (t - 1) * batch_size\n",
    "            n = t * batch_size\n",
    "            batch_list = tickers_list[m:n]\n",
    "            if show_batch:\n",
    "                print(batch_list,m,n)\n",
    "            batch_download = yf.download(tickers= batch_list,start= start, end = end, interval = \"1d\",\n",
    "                                group_by = 'column',auto_adjust = True, prepost = True)['Close']\n",
    "            px_df = px_df.join(batch_download, how='outer')\n",
    "        except:\n",
    "            pass\n",
    "    return px_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate df_abs\n",
    "2. Generate df_rel of relative prices by dividing the df_abs by the benchmark ticker and rebase to the first benchmark value \n",
    "3. Print the shape of the absolute and relative df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 51 ; start= '2001-01-01' ; end = None\n",
    "\n",
    "df_abs = batch_px_df(tickers_list,batch_size, start, end,show_batch = False).round(0)\n",
    "df_rel = df_abs.dropna(subset=[bm_ticker], axis=0)\n",
    "df_rel = round(df_abs.divide(df_abs[bm_ticker], axis=0).mul(df_abs[df_abs[bm_ticker].notna()].iloc[0,list(df_abs.columns).index(bm_ticker)]),2)\n",
    "df_abs.shape, df_rel.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bullish/Bearish regime in absolute/relative series using 1 year breakout/breakdown\n",
    "\n",
    "We use a classic one-year high/low as a proxy for the bullish/bearish regime. Stocks that have printed a 252 one-year high are deemed bullish and expected to remain strong. Conversely, stocks that have printed a one-year low are expected to exhibit continued weakness. \n",
    "\n",
    "1. Instantiate a bullbear df. Copying the benchmark price and calculate cumulative returns\n",
    "2. generate 2 dataframes from absolute and relative price series:\n",
    "    1. np.where price hits a 252 day high, assign 1 for bullish\n",
    "    2. np.where price hits a 252 day low, assign -1 for bearish\n",
    "    3. else assign NaN\n",
    "    4. forward fillna to propagate bullish or bearish regime\n",
    "3. Horizontal count of stocks in bullish or bearish territory. Calculate average lateral bullishness/bearishness for both absolute and relative series\n",
    "4. Plot benchmark and lateral counts for the absolute, reative abs & rel, average absolute and relative\n",
    "\n",
    "Absolute series are highly correlated to the benchmark. It is not uncommon to have close to 100% of the consituents be in either bullish or bearish territory. However, with relative series, roughly 40% to 60% of the constituents will be in either bullish or bearish territory at any point in time. This means there are more issues to shop from and bild a portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 252 # 1 year rolling window\n",
    "bullbear = pd.DataFrame()\n",
    "bullbear['benchmark'] = df_abs[bm_ticker].copy()\n",
    "bullbear['bm_returns'] = np.exp(np.log(bullbear/bullbear.shift()).cumsum()) - 1\n",
    "\n",
    "bo_abs = pd.DataFrame(data= np.where(df_abs >= df_abs.rolling(window).max(),1,\n",
    "                                     np.where(df_abs<=df_abs.rolling(window).min(),-1,np.nan)), \n",
    "                      index= df_abs.index,columns= df_abs.columns).ffill()\n",
    "bo_abs = bo_abs.dropna(how = 'all', axis=0)\n",
    "\n",
    "bo_rel = pd.DataFrame(data = np.where(df_rel >= df_rel.rolling(window).max(),1,\n",
    "                                np.where(df_rel <= df_rel.rolling(window).min(),-1,np.nan)), \n",
    "                      index= df_rel.index,columns= df_rel.columns).ffill()\n",
    "bo_rel = bo_rel.dropna(how = 'all', axis=0)\n",
    "\n",
    "bo_abs_count = bo_abs.count(axis=1) \n",
    "bo_rel_count = bo_rel.count(axis=1) \n",
    "bullbear['bulls_absolute'] = bo_abs[bo_abs > 0].count(axis=1).div(bo_abs_count,fill_value=0) * 100\n",
    "bullbear['bears_absolute'] = bo_abs[bo_abs < 0].count(axis=1).div(bo_abs_count,fill_value=0) * 100 \n",
    "bullbear['bulls_relative'] = bo_rel[bo_rel > 0].count(axis=1).div(bo_rel_count,fill_value=0) * 100  \n",
    "bullbear['bears_relative'] = bo_rel[bo_rel < 0].count(axis=1).div(bo_rel_count,fill_value=0) * 100  \n",
    "bullbear['bullbear_absolute_avg'] = bo_abs.mean(axis=1) \n",
    "bullbear['bullbear_relative_avg'] = bo_rel.mean(axis=1) \n",
    "bullbear = round(bullbear,2).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullbear[280:][['bm_returns','bulls_absolute', 'bears_absolute']].plot(figsize=(15,4),\n",
    "                style=['k','g', 'r'],grid=True,secondary_y=['bm_returns'],\n",
    "                title = 'S&P500: 1 year Bullish/Bearish count, absolute series')\n",
    "\n",
    "bullbear[280:][['bm_returns','bulls_relative', 'bears_relative']].plot(figsize=(15,4),\n",
    "                style=['k','g-.', 'r-.'],grid=True,secondary_y=['bm_returns'],\n",
    "                title = 'S&P500: 1 year Bullish/Bearish count, relative series')\n",
    "\n",
    "bullbear[280:][['bm_returns','bulls_absolute', 'bears_absolute', 'bulls_relative', 'bears_relative']].plot(figsize=(15,4),\n",
    "                style=['k','g', 'r','g-.', 'r-.'],grid=True,secondary_y=['bm_returns'],\n",
    "                title = 'S&P500: 1 year Bullish/Bearish count, absolute & relative series')\n",
    "\n",
    "bullbear[280:][['bm_returns','bullbear_absolute_avg','bullbear_relative_avg']].plot(figsize=(15,4),\n",
    "                style=['k', 'b', 'm'],grid=True,secondary_y=['bm_returns'],\n",
    "                title = 'S&P500: 1 year Bullish/Bearish average absolute & relative series')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Relative  & Utilities Function\n",
    "\n",
    "1. rel_fx(df,_o,_h,_l,_c,bm_df,bm,ccy_df,ccy,start,end,rebase=True,mult=1):\n",
    "\n",
    "This function converts absolute local price series into fund currency relative to a benchmark. It is a 3 step-process \n",
    "    1. Pull benchmark and currency data from their dataframes called bm_df and ccy_df, respectively\n",
    "    2. Convert OHLC price to benchmark currency by dividing by the currency rate  \n",
    "    3. Divide by the benchmark: \n",
    "        1. rebase = True. This rebases the multiplier to the beginning of the series. This works best when the start of the series is a fixed date. \n",
    "        2. rebase = False. This is better suited for continuous series or moving windows such as rolling 1,2, 3 years    \n",
    "\n",
    "2. dict_swap(dct): this dictionary comprehension swaps keys and values\n",
    "\n",
    "3. df_from_dict(batch_size,dct,start,end): this function downloads historical prices from dictionary keys,\n",
    " and swaps column names with values\n",
    "\n",
    "4. rohlc(df,relative = False): this verbose function instantiates _o,_h,_l,_c\n",
    "\n",
    "5. def yf_droplevel(batch_download,ticker): drops multiindex df into single ticker df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_fx(df,_o,_h,_l,_c, bm_df, bm, ccy_df, ccy, start, end,rebase = True, mult = 1000):\n",
    "    df[ccy] = ccy_df.loc[start:end,ccy].copy().ffill()\n",
    "    df[bm] = bm_df.loc[start:end,bm].copy().ffill()\n",
    "    for i in [_o,_h,_l,_c]:\n",
    "        df[f'r{i}'] = df[i].div(df[ccy])\n",
    "        if rebase == True:\n",
    "            df[f'r{i}'] = df[f'r{i}'].div(df[bm]).mul(df[df[bm].notna()].iloc[0,list(df.columns).index(bm)])\n",
    "        else:\n",
    "            df[f'r{i}'] = df[f'r{i}'].div(df[bm]) * mult\n",
    "    return df\n",
    "\n",
    "def dict_swap(dct):\n",
    "    dict_swap = {value:key for key,value in dct.items()}\n",
    "    return dict_swap\n",
    "\n",
    "def df_from_dict(batch_size,dct,start,end):\n",
    "    tickers_list = list(dct.keys())\n",
    "    df = batch_px_df(tickers_list,batch_size, start, end,show_batch = False)\n",
    "    df = df.rename(columns = dct)\n",
    "    df = df.tz_localize(None).ffill()\n",
    "    return df\n",
    "\n",
    "def rohlc(df,relative = False):\n",
    "    if relative==True:\n",
    "        rel = 'r'\n",
    "    else:\n",
    "        rel= ''      \n",
    "    if 'Open' in df.columns:\n",
    "       _o,_h,_l,_c = f'{rel}Open',f'{rel}High',f'{rel}Low',f'{rel}Close'      \n",
    "    elif 'open' in df.columns:\n",
    "        _o,_h,_l,_c = f'{rel}open',f'{rel}high',f'{rel}low',f'{rel}close'\n",
    "    else:\n",
    "        _o=_h=_l=_c= np.nan\n",
    "    return _o,_h,_l,_c\n",
    "\n",
    "def yf_droplevel(batch_download,ticker):\n",
    "    df = batch_download.iloc[:, batch_download.columns.get_level_values(1) == ticker]\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Softbank in absolute JPY & relative to Nasdaq 100 in USD\n",
    "\n",
    "1. We create dictionaries of benchmarks and currencies. We download historical prices. We add 'absolute' and 'local' with 1 values. The bm_df and ccy_df will be re-purposed later\n",
    "2. We download historical prices for Softbank, instantiate _o,_h,_l,_c, calculate relative prices and plot both absolute in local currency and relative is USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_bm = {'^GSPC': 'SP500', '^DJI': 'DOW30', '^IXIC': 'Nasdaq100', '^NYA': 'NYSE ', '^XAX': 'Nyse_Amex', '^RUT': 'Russell2000',\n",
    "           '^VIX': 'VIX', '^BUK100P': 'CBOEUK100', '^FTSE': 'FTSE100', '^GDAXI': 'DAX', '^FCHI': 'CAC40', '^STOXX50E': 'EuroStocks50', \n",
    "           '^N100': 'Euronext100', '^BFX': 'BEL20', 'IMOEX.ME': 'MOEX_Russia', '^N225': 'Nikkei225', '^HSI': 'HangSeng', \n",
    "           '000001.SS': 'SSE', '399001.SZ': 'Shenzhen', '^STI': 'STI', '^AXJO': 'ASX200', '^AORD': 'All_Ordinaries', '^BSESN': 'Sensex', \n",
    "           '^JKSE': 'Jakarta','^KLSE': 'FTSE_KLCI', '^JKSE': 'Jakarta', '^KLSE': 'FTSE_KLCI', '^NZ50': 'NZX50', '^KS11': 'Kospi', \n",
    "           '^TWII': 'TSEC', '^GSPTSE': 'TSX', '^BVSP': 'Bovespa',  '^MXX': 'IPC_Mexico', '^MERV': 'Merval', '^TA125.TA': 'TA125',\n",
    "           '^IRX': '3M_US_Yield', '^FVX': '5Y_US_Yield', '^TNX': '10Y_US_Yield', '^TYX': '30Y_US_Yield'}\n",
    "bm_df = df_from_dict(batch_size,dict_bm,start,end)\n",
    "bm_df['absolute'] = 1\n",
    "\n",
    "dict_ccy = {'EURUSD=X': 'EURUSD', 'JPY=X': 'USDJPY', 'GBPUSD=X': 'GBPUSD', 'AUDUSD=X': 'AUDUSD', 'NZDUSD=X': 'NZDUSD', 'USDCAD=X': 'USDCAD',\n",
    "            'EURJPY=X': 'EURJPY', 'GBPJPY=X': 'GBPJPY', 'EURGBP=X': 'EURGBP', 'EURCAD=X': 'EURCAD', 'EURSEK=X': 'EURSEK', 'EURCHF=X': 'EURCHF',\n",
    "             'EURHUF=X': 'EURHUF','CNY=X': 'USDCNY', 'HKD=X': 'USDHKD','SGD=X': 'USDSGD','INR=X': 'USDINR', 'KRW=X': 'USDKRW','TWD=X': \n",
    "            'USDTWD', 'MXN=X': 'USDMXN', 'PHP=X': 'USDPHP', 'IDR=X': 'USDIDR', 'THB=X': 'USDTHB', 'MYR=X': 'USDMYR', 'ZAR=X': 'USDZAR',\n",
    "            'RUB=X': 'USDRUB', 'BRL=X': 'USDBRL'}\n",
    "ccy_df = df_from_dict(batch_size,dict_ccy,start,end)\n",
    "ccy_df['local'] = 1\n",
    "\n",
    "bm = 'Nasdaq100' ; ccy = 'USDJPY' ; dgt = 2\n",
    "ticker = '9984.T' # Softbank\n",
    "df =  yf.download(tickers= ticker,start= start, end = end, interval = \"1d\",group_by = 'column',\n",
    "                        auto_adjust = True, prepost = True)\n",
    "df = round(yf_droplevel(df,ticker),2).tz_localize(None).ffill()\n",
    "\n",
    "_o,_h,_l,_c = rohlc(df,relative = False)\n",
    "df = rel_fx(df,_o,_h,_l,_c, bm_df, bm, ccy_df, ccy, start, end,rebase = True, mult = 1000)\n",
    "\n",
    "df[-1300:][['Close','rClose']].plot(figsize=(15,4),grid=True, secondary_y = ['rClose'],title= 'Softbank Absolute in JPY vs relative to Nasdaq in USD' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. GEMS Stocks relative to S&P 500\n",
    "\n",
    "1. Tickers list + benchmark\n",
    "2. Download historical close prices\n",
    "3. Harmonise dataframe: 1) restate to local time zone, 2) forward fill 3) drop NaN to start when every price is populated \n",
    "4. Relative series gems_rel : 1) Divide by benchmark close prices 2) rebase to the first value of the benchmark\n",
    "5. Rename columns of gems rel usin a list comprehension\n",
    "6. Concatenate absolute and relative dataframes\n",
    "7. Calculate returns and plot with secondary axis for relative series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = ['^GSPC'] ; tickers_list = ['GE','GM','GS'] \n",
    "tickers = benchmark + tickers_list\n",
    "\n",
    "gems = yf.download(tickers,start, end, interval = \"1d\",group_by = 'column',auto_adjust = True, prepost = True)['Close']\n",
    "gems = gems.tz_localize(None).ffill().dropna(how='any',axis=0)\n",
    "gems_rel = gems[tickers_list].div(gems[bm_ticker], axis=0).mul(gems.iloc[0,list(gems.columns).index(bm_ticker)])\n",
    "gems_rel.columns = ['rel_' + i for i in tickers_list]\n",
    "gems = round(pd.concat([gems,gems_rel],axis=1),2)\n",
    "gems_returns = np.exp(np.log(gems/gems.shift()).cumsum()) - 1\n",
    "\n",
    "gems_returns.plot(figsize=(15,4),grid=True,style = ['k','b','g','r','b-.','g-.','r-.'],\n",
    "                    secondary_y = list(gems_rel.columns), title = 'GEMS: GE, GM, GS, Absolute & Relative returns v. S&P500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Sector Average\n",
    "\n",
    "We previously looked at the broad market number of bullish/bearish issues. Next, letâ€™s drill down and look at what happens at the sector level.\n",
    "We will concatenate one specific column from the df_SP500 dataframe with the historical regime df. We will calculate the sector average using a pivot table.\n",
    "\n",
    "1. drop tickers without values\n",
    "2. We rename the the columns 'ticker'\n",
    "3. We concatenate one target column (sector, industry, sub-industry, etc) of df_SP500 with the transposed df_historical\n",
    "4. We calculate the average of each sector score using pivot_table, then transpose\n",
    "5. We calculate sector average for both the absolute and relative series\n",
    "6. We concatenate witht he bullbear df adding the suffix \"_abs\" and \"_rel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sector_avg_df(df_table, target_col, df_historical):\n",
    "    \"\"\"\n",
    "    Calculate the average absolute bullish/bearish count for each sector.\n",
    "    \"\"\"\n",
    "    df_historical = df_historical.copy().dropna(how='all', axis=1)    \n",
    "    df_historical.columns.names = ['ticker']\n",
    "    sector_df = pd.concat([df_table[[target_col]], df_historical.T], axis=1)\n",
    "    sector_avg_df = pd.pivot_table(sector_df, values=list(df_historical.index), index=[target_col], aggfunc=\"mean\").T\n",
    "    return sector_avg_df\n",
    "\n",
    "sector_avg_abs = round(sector_avg_df(df_SP500, 'sector', bo_abs).round(2),2)\n",
    "sector_avg_rel = round(sector_avg_df(df_SP500, 'sector', bo_rel).round(2),2)\n",
    "bullbear = pd.concat([bullbear, sector_avg_abs.add_suffix('_abs'), sector_avg_rel.add_suffix('_rel')], axis=1)\n",
    "\n",
    "bullbear.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = df_SP500['sector'].unique().tolist()\n",
    "sector_avg_abs_cols = [i+'_abs' for i in sectors]\n",
    "sector_avg_rel_cols = [i+'_rel' for i in sectors]\n",
    "\n",
    "bullbear[['bm_returns', 'bullbear_absolute_avg']+sector_avg_abs_cols].plot(figsize= (15,4),secondary_y =['bm_returns'],style = ['k','b*'],\n",
    "                                                                           title = 'S&P500, sectors in absolute')\n",
    "\n",
    "bullbear[['bm_returns', 'bullbear_relative_avg']+sector_avg_rel_cols].plot(figsize= (15,4),secondary_y =['bm_returns'],style = ['k','g*'],\n",
    "                                                                           title = 'S&P500, sectors in relative')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Cyclicals v. defensives\n",
    "\n",
    "1. Define defensives and cyclicals lists\n",
    "2. We calculate the average of those two sectors. These are equal weight averages. \n",
    "3. We plot the defensives and cyclicals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defensives_list = ['Consumer Staples_rel', 'Consumer Staples_abs','Utilities_rel', 'Utilities_abs']\n",
    "bullbear['defensives_rel'] = bullbear[ ['Consumer Staples_rel','Utilities_rel']].mean(axis=1)\n",
    "bullbear['defensives_abs'] = bullbear[['Consumer Staples_abs','Utilities_abs']].mean(axis=1)\n",
    "defensives_plot =['benchmark'] + defensives_list +['defensives_rel','defensives_abs'] \n",
    "                   \n",
    "cyclicals_list = ['Consumer Discretionary_rel', 'Consumer Discretionary_abs',\n",
    "                   'Information Technology_rel', 'Information Technology_abs',]\n",
    "bullbear['cyclicals_abs'] = bullbear[[ 'Consumer Discretionary_abs', 'Information Technology_abs']].mean(axis=1)          \n",
    "bullbear['cyclicals_rel'] = bullbear[[ 'Consumer Discretionary_rel', 'Information Technology_rel']].mean(axis=1)\n",
    "cyclicals_plot = ['benchmark'] + cyclicals_list + ['cyclicals_rel','cyclicals_abs']\n",
    "\n",
    "bullbear['rotation_rel'] = bullbear['cyclicals_rel'].sub(bullbear['defensives_rel'], fill_value =0)                   \n",
    "bullbear['rotation_abs'] = bullbear['cyclicals_abs'].sub(bullbear['defensives_abs'], fill_value =0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullbear[window:][defensives_plot].plot(figsize=(15,4),grid=True,secondary_y=['benchmark'],\n",
    "            style = ['k','b','b:','g','g:','co-','c+-'],title = 'Defensive sectors in absolute & relative')\n",
    "\n",
    "bullbear[window:][cyclicals_plot].plot(figsize=(15,4),grid=True,secondary_y=['benchmark'],\n",
    "            style = ['k','m','m:','r','r:','yo-','y+-'],title = 'Cyclical sectors in absolute & relative')\n",
    "\n",
    "bullbear[window:][['benchmark', 'rotation_rel', 'cyclicals_rel','defensives_rel'\n",
    "                   ]].plot(figsize=(15,4),grid=True,secondary_y=['benchmark'],\n",
    "            style = ['k','r','y','c'], title = str('benchmark, rotation_rel, cyclicals_rel, defensives_rel') )\n",
    "                   \n",
    "bullbear[window:][['benchmark', 'rotation_abs', 'cyclicals_abs','defensives_abs'\n",
    "                   ]].plot(figsize=(15,4),grid=True,secondary_y=['benchmark'],\n",
    "            style = ['k','r:','y:','c:'],title = str('benchmark, rotation_abs, cyclicals_abs, defensives_abs') )  \n",
    "                   \n",
    "bullbear[window:][['benchmark', 'rotation_rel','rotation_abs']].plot(figsize=(15,4),grid=True,secondary_y=['benchmark'],\n",
    "            style = ['k','r','r:'],title = str('benchmark, rotation_rel, rotation_abs') )     \n",
    "\n",
    "bullbear[window:][['bm_returns', 'rotation_rel','rotation_abs']].plot(figsize=(15,4),grid=True,secondary_y=['bm_returns'],\n",
    "            style = ['k','r','r:'],title = str('bm_returns, rotation_rel, rotation_abs') )     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Cross-market comparison\n",
    "\n",
    "The relative method is particulary useful to compare stocks in the same sectors. This is a three step process:\n",
    "1. Build the currency dictionary, download hsitorical data. \n",
    "2. Convert into benchmark currency\n",
    "3. Calculate returns in benchmark currency for the absolute and relative series\n",
    "\n",
    "Step1:\n",
    "1. We build a dictionary fo tickers and their correpsponding currency\n",
    "2. We generate a tickers list from the dictionary\n",
    "3. We download historical prices in local currency\n",
    "4. We add the benchmark prices from the bm_df we built earlier\n",
    "5. We forward fill missing values for the entire df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_dict = {'7203.T': 'USDJPY','VOW3.DE' :'EURUSD', 'RNO.PA':'EURUSD','F':'local' ,'TSLA':'local' ,'GM':'local' ,'005380.KS':'USDKRW' }\n",
    "tickers = list(fx_dict.keys()) ; bm = 'SP500' ; start = '2021-12-30'\n",
    "px_df = pd.DataFrame()\n",
    "px_df = yf.download(tickers,start, end, interval = \"1d\",\n",
    "                    group_by = 'column',auto_adjust = True, prepost = True)['Close']\n",
    "px_df[bm] = bm_df[start:][[bm]].copy()\n",
    "px_df = round(px_df.tz_localize(None).ffill(),2)\n",
    "px_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We calculate daily log returns\n",
    "2. We calculate cumulative returns using expanding().sum() instead of cumsum() to circumvent NaN\n",
    "3. We calculate relative cumulative returns in local currency\n",
    "4. We copy the local price df to instantiate the currency adjusted px_df_usd\n",
    "5. We loop through the fx_dict to divide the px_df by the corresponding currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_log_returns_abs = np.log(px_df/px_df.shift())\n",
    "cumul_returns_abs = daily_log_returns_abs.expanding().sum().apply(np.exp) - 1\n",
    "daily_log_returns_rel = daily_log_returns_abs.sub(daily_log_returns_abs[bm], axis=0)\n",
    "cumul_returns_rel = daily_log_returns_rel.expanding().sum().apply(np.exp) - 1\n",
    "\n",
    "cumul_returns_abs.plot(figsize=(15,4),grid=True,secondary_y = [bm],style = ['k','b','g','r','c','m'],\n",
    "                       title = 'Cumulative returns and S&P500 in LOCAL currency, absolute series')\n",
    "cumul_returns_rel.plot(figsize=(15,4),grid=True,secondary_y = [bm],style = ['k','b','g','r','c','m'],\n",
    "                       title = 'Cumulative returns relative to S&P500 in LOCAL currency')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a dictionary comprehension using px_df key divided by the corresponding currency value\n",
    "2. Generate a df px_df_usd from the dictionary\n",
    "3. Import benchmark\n",
    "4. We localize timezone to align benchmark and px_df_usd, drop NaN and re-order columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "px_dict_usd = {key:px_df[key].div(ccy_df[value]).ffill() for key,value in fx_dict.items()}\n",
    "px_df_usd = pd.DataFrame(px_dict_usd)\n",
    "px_df_usd[bm] = bm_df[start:][[bm]].copy()\n",
    "px_df_usd = round(px_df_usd.tz_localize(None).ffill(),2)\n",
    "px_df_usd = px_df_usd.dropna(subset = fx_dict.keys())\n",
    "px_df_usd = px_df_usd[px_df.columns]\n",
    "px_df_usd.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_log_returns_abs_usd = np.log(px_df_usd/px_df_usd.shift())\n",
    "cumul_returns_abs_usd = daily_log_returns_abs_usd.expanding().sum().apply(np.exp) - 1\n",
    "daily_log_returns_rel_usd = daily_log_returns_abs_usd.sub(daily_log_returns_abs_usd[bm], axis=0)\n",
    "cumul_returns_rel_usd = round(daily_log_returns_rel_usd.expanding().sum().apply(np.exp) - 1,4)\n",
    "\n",
    "cumul_returns_abs_usd.plot(figsize=(15,4),secondary_y = [bm],style = ['k','b','g','r','c','m'],\n",
    "                       grid= True, title = 'cumulative return in absolute, in USD')\n",
    "\n",
    "cumul_returns_rel_usd.plot(figsize=(15,4),secondary_y = [bm],style = ['k','b','g','r','c','m'],grid=True,\n",
    "                                   title = 'cumulative returns relative to S&P 500, in USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a heatmap of returns\n",
    "\n",
    "def returns_table(daily_log_returns, range_days): We build a table of returns for a universe of securities over various time periods\n",
    "1. We convert the index of daily_log_returns to time. Today's date is logically the last row\n",
    "2. We use relativedelta from today's date to calculate the cumulative sum of the returns\n",
    "3. We apply np.exp and subtract 1 to convert log returns into geometric returns\n",
    "4. We calculate the range from a set period \n",
    "\n",
    "def heatmap(df, subset_cols, clrmap = 'RdYlGn', prec =1):    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_table(daily_log_returns, range_days):\n",
    "    daily_log_returns.index = pd.to_datetime(daily_log_returns.index)\n",
    "    dt_today = daily_log_returns.index.max()\n",
    "\n",
    "    rtrns_df = pd.DataFrame()\n",
    "    rtrns_df['1D'] = daily_log_returns.ffill()[-1:].sum()\n",
    "    rtrns_df['1W'] = daily_log_returns[dt_today + relativedelta(weeks = - 1):].sum()\n",
    "    rtrns_df['1M'] = daily_log_returns[dt_today + relativedelta(months = - 1):].sum()\n",
    "    rtrns_df['3M'] = daily_log_returns[dt_today + relativedelta(months = - 3):].sum()\n",
    "    rtrns_df['6M'] = daily_log_returns[dt_today + relativedelta(months = - 6):].sum()\n",
    "    rtrns_df['1Y'] = daily_log_returns[dt_today + relativedelta(years = -1):].sum()\n",
    "    rtrns_df['2Y'] = daily_log_returns[dt_today + relativedelta(years = -2):].sum()\n",
    "    rtrns_df['3Y'] = daily_log_returns[dt_today + relativedelta(years = -3):].sum()\n",
    "    rtrns_df['Max'] = daily_log_returns.sum()\n",
    "    rtrns_df = round((np.exp(rtrns_df)-1)*100,1)\n",
    "    cumsum = daily_log_returns[dt_today + relativedelta(days = - range_days):].cumsum()\n",
    "    rng = round((cumsum.tail(1) - cumsum.min()) / (cumsum.max() - cumsum.min()),2)\n",
    "    rtrns_df = pd.concat([rtrns_df,rng.T],axis=1)\n",
    "    rtrns_df =rtrns_df.rename(columns={rtrns_df.columns[-1]: f'range {range_days}d'})\n",
    "    return rtrns_df\n",
    "\n",
    "def heatmap(df, subset_cols, clrmap = 'RdYlGn', prec =1):    \n",
    "    try:\n",
    "        return df.style.background_gradient(subset= subset_cols,cmap= clrmap).format(precision = prec)\n",
    "    except:\n",
    "        return df.style.background_gradient(subset= subset_cols,cmap= clrmap).set_precision(prec)\n",
    "\n",
    "returns_table_abs = returns_table(daily_log_returns_abs, range_days=364)\n",
    "returns_table_rel_usd = returns_table(daily_log_returns_rel_usd, range_days=364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_period = ['6M']\n",
    "heatmap(returns_table_abs.sort_values(by=sort_period, ascending = False), \n",
    "        subset_cols = returns_table_abs.columns, clrmap = 'RdYlGn', prec =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(returns_table_rel_usd.sort_values(by=sort_period, ascending = False), \n",
    "        subset_cols = returns_table_rel_usd.columns, clrmap = 'RdYlGn', prec =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save df and read: NO need to publish in the book version\n",
    "returns_table_abs.sort_values(by=sort_period, ascending = False).to_csv('returns_table_abs.csv')\n",
    "returns_table_rel_usd.sort_values(by=sort_period, ascending = False).to_csv('returns_table_rel_usd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Disney MCU rise and fall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'DIS' ; bm = 'SP500'; ccy = 'local' ;\n",
    "df = yf.download(ticker,start = '2017-12-30', end = None ,interval = \"1d\",group_by = 'column',auto_adjust = True)\n",
    "df = yf_droplevel(df,ticker)\n",
    "start = '2019-12-30'; end= None\n",
    "df1 = rel_fx(df,_o,_h,_l,_c, bm_df, bm, ccy_df, ccy, start, end,rebase = True, mult = 1)\n",
    "df2 = rel_fx(df,_o,_h,_l,_c, bm_df, bm, ccy_df, ccy, start, end,rebase = False, mult = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = rel_fx(df,_o,_h,_l,_c, bm_df, bm, ccy_df, ccy, start, end,rebase = True, mult = 100)\n",
    "df1[[bm,'Close','rClose']].plot(figsize=(15,4),grid=True, secondary_y = [bm], \n",
    "                                style = ['grey','k', 'r' ],title= f'{ticker} Absolute vs Relative series rebased')\n",
    "\n",
    "df2 = rel_fx(df,_o,_h,_l,_c, bm_df, bm, ccy_df, ccy, start, end,rebase = False, mult = 2000)\n",
    "df2[[bm,'Close','rClose']].plot(figsize=(15,4),grid=True, secondary_y = [bm], \n",
    "                                style = ['grey','k', 'r' ],title= f'{ticker} Absolute vs Relative series continuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
