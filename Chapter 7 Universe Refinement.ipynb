{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c97c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "import yfinance as yf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045642f0",
   "metadata": {},
   "source": [
    "1. Liquidity: currency of the bear market\n",
    "    1. market impact: stay mid-large caps on the short side\n",
    "    2. Market Cap: function and fields\n",
    "2. Exchange: OTC\n",
    "4. Crowded shorts: \n",
    "    1. Percentage of the free float\n",
    "    2. ratios and prudence\n",
    "\n",
    "4. Valuations\n",
    "    1. P/E : poor predictor of imminent shorts. Value traps are often cheap\n",
    "    2. High PBR indicate highly leveraged balance sheet. Everything is about timing\n",
    "    3. Dividend yield:\n",
    "        1. avg 5 year and underperformance\n",
    "        2. navigate ex-date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798b137",
   "metadata": {},
   "source": [
    "### Downloading information for all S&P 500 companies\n",
    "\n",
    "How it works:\n",
    "1. Fetch data: Request Wikipedia page with browser headers to avoid blocking\n",
    "2. Extract table: Parse HTML and select the second table [1] containing company data\n",
    "3. Clean up: Rename 'GICS Sector' → 'sector' and set ticker symbols as index\n",
    "4. Verify: Display first 5 rows\n",
    "5. Result: DataFrame indexed by ticker (e.g., AAPL, MSFT) with columns for company name, sector, date added, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}  # (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "response= requests.get(url, headers=headers)\n",
    "df_SP500 = pd.read_html(StringIO(response.text))[0]\n",
    "df_SP500 = df_SP500.rename(columns={'GICS Sector':'sector'})\n",
    "df_SP500.set_index('Symbol', inplace=True)\n",
    "\n",
    "bm_ticker = '^GSPC'\n",
    "tickers = [bm_ticker] + df_SP500.index.tolist() \n",
    "df_SP500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_px_df(tickers_list,batch_size, start, end,show_batch = False):\n",
    "    px_df = pd.DataFrame()\n",
    "    loop_size = int(len(tickers_list) // batch_size) + 2\n",
    "    for t in range(1,loop_size): # Batch download\n",
    "        try:\n",
    "            m = (t - 1) * batch_size\n",
    "            n = t * batch_size\n",
    "            batch_list = tickers_list[m:n]\n",
    "            if show_batch:\n",
    "                print(batch_list,m,n)\n",
    "            batch_download = yf.download(tickers= batch_list,start= start, end = end, interval = \"1d\",\n",
    "                                group_by = 'column',auto_adjust = True, prepost = True)['Close']\n",
    "            px_df = px_df.join(batch_download, how='outer')\n",
    "        except:\n",
    "            pass\n",
    "    return px_df.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a697211",
   "metadata": {},
   "source": [
    "### Fetch detailed company information for all S&P 500 tickers from YFinance\n",
    "\n",
    "How it works:\n",
    "1. Loop through tickers: Iterate over each S&P 500 symbol\n",
    "2. Fetch data: Use yf.Ticker(symbol).info to get company fundamentals (market cap, PE ratio, dividends, short interest, etc.)\n",
    "3. Error handling: If fetch fails, store None and print error message\n",
    "4. Build DataFrame: Convert dictionary to DataFrame with tickers as index\n",
    "\n",
    "Result: DataFrame with ~500 rows (one per company) and 100+ columns containing financial metrics, ratios, and company metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ca1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_info_dict = {}\n",
    "\n",
    "for symbol in tickers:\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        info = ticker.info\n",
    "        sp500_info_dict[symbol] = info\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching info for {symbol}: {e}\")\n",
    "        sp500_info_dict[symbol] = None\n",
    "        \n",
    "sp500_info_df= pd.DataFrame.from_dict(sp500_info_dict, orient='index')\n",
    "# sp500_info_df.to_csv('sp500_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb78782",
   "metadata": {},
   "source": [
    "### Liquidity is the currency of bear markets\n",
    "\n",
    "Clean and engineer key screening metrics for short selling: liquidity, market cap, and short interest indicators.\n",
    "\n",
    "How it works:\n",
    "1. Date formatting: Convert dividend dates from Unix timestamps to YYYY-MM-DD format\n",
    "2. Market cap fix: Calculate market cap from shares × price when API value is zero; convert to millions\n",
    "3. Liquidity metric: Compute daily dollar volume traded (prefer 10-day average over standard average)\n",
    "4. Short interest: Calculate short % of float from raw shares data; fill missing values from API field\n",
    "5. Display subset: Show key columns for liquidity, shorts, valuations, and dividends\n",
    "\n",
    "Result: Cleaned DataFrame with standardized metrics ready for screening high-liquidity, heavily-shorted, or dividend-paying stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfcdeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_info_df[['exDividendDate', 'dividendDate']] = sp500_info_df[['exDividendDate', 'dividendDate']].apply(lambda x: pd.to_datetime(x).dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "# Market Cap correction\n",
    "sp500_info_df['marketCap_calculated'] = sp500_info_df['sharesOutstanding'].mul(sp500_info_df['currentPrice'])\n",
    "sp500_info_df['market_Cap']  = np.where(sp500_info_df['marketCap'] == 0, sp500_info_df['marketCap_calculated'], sp500_info_df['marketCap'])\n",
    "sp500_info_df['market_Cap']  = round(sp500_info_df['market_Cap'] *1e-6)\n",
    "\n",
    "# Average Liquidity calculation\n",
    "value_traded  = round(sp500_info_df['averageVolume'].mul(sp500_info_df['currentPrice']) *1e-6)\n",
    "value_traded_avg10  = round(sp500_info_df['averageDailyVolume10Day'].mul(sp500_info_df['currentPrice']) *1e-6)\n",
    "sp500_info_df['value_traded'] = np.where(~pd.isna(value_traded_avg10), value_traded_avg10, value_traded)\n",
    "\n",
    "# Crowded shorts\n",
    "sp500_info_df['shortfloat'] = round(sp500_info_df['sharesShort'].div(sp500_info_df['floatShares']),3)\n",
    "sp500_info_df['shortPctFloat'] = np.where(~pd.isna(sp500_info_df['shortfloat']), \n",
    "                                          sp500_info_df['shortfloat'], sp500_info_df['shortPercentOfFloat'])\n",
    "\n",
    "sp500_info_df[['shortName','sector','exchange','market_Cap', 'value_traded', 'currentPrice',#'sharesOutstanding', 'averageDailyVolume10Day', 'averageVolume',\n",
    " 'shortPctFloat', 'shortRatio', #'sharesShort', 'floatShares', 'shortPercentOfFloat',\n",
    "'exDividendDate', 'dividendDate','dividendYield', 'fiveYearAvgDividendYield', 'payoutRatio',\n",
    "'trailingPE', 'forwardPE', 'priceToBook', ]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a154a15",
   "metadata": {},
   "source": [
    "### Identify the most heavily shorted stocks by short ratio\n",
    "\n",
    "Screen for stocks with the highest short ratio (days to cover) to find crowded short positions that may face squeeze risk.\n",
    "\n",
    "How it works:\n",
    "1. Rank stocks: Calculate percentile rank of shortRatio across all S&P 500 stocks\n",
    "2. Sort and select: Sort descending and take top 10 tickers with highest ratios\n",
    "3. Display metrics: Show shortPctFloat and shortRatio for these heavily-shorted names\n",
    "4. Interpretation: High short ratio means many days of average volume needed to cover all short positions\n",
    "\n",
    "Result: Top 10 stocks where shorts may face difficulty exiting positions quickly, indicating potential squeeze candidates or genuine distress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ratio = sp500_info_df['shortRatio'].rank(pct=True).sort_values(ascending=False).index[:10].tolist()\n",
    "sp500_info_df.loc[short_ratio, ['shortName','sector','shortPctFloat', 'shortRatio']].round(3).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_info_df.groupby('sector')[['shortPctFloat','shortRatio']].mean().sort_values(by='shortPctFloat', ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4d244",
   "metadata": {},
   "source": [
    "### Identify the most heavily shorted stocks by short as a percentage of free float\n",
    "\n",
    "Screen for stocks with the highest short as a percentage of free float to find crowded short positions that may face squeeze risk.\n",
    "\n",
    "How it works:\n",
    "1. Rank stocks: Calculate percentile rank of shortRatio across all S&P 500 stocks\n",
    "2. Sort and select: Sort descending and take top 10 tickers with highest ratios\n",
    "3. Display metrics: Show shortPctFloat and shortRatio for these heavily-shorted names\n",
    "4. Interpretation: High short as a percentage of free float means demand for short (shares borrowed) has increased as a percentage of the supply (free float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_short_float = sp500_info_df['shortPercentOfFloat'].rank(pct=True).sort_values(ascending=False).index[:10].tolist()\n",
    "sp500_info_df.loc[high_short_float, ['shortName','sector','shortPercentOfFloat', 'shortRatio']].round(3).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779bb20",
   "metadata": {},
   "source": [
    "### Analyze dividend metrics by sector\n",
    "\n",
    "Compare dividend characteristics across sectors to identify high-yield sectors and assess dividend sustainability.\n",
    "\n",
    "How it works:\n",
    "1. Group by sector: Aggregate all S&P 500 stocks by GICS sector\n",
    "2. Calculate means: Compute average dividendYield, fiveYearAvgDividendYield, and payoutRatio per sector\n",
    "3. Sort by 5-year average: Rank sectors from highest to lowest historical dividend yield\n",
    "\n",
    "Interpretation: In chapter 3 we calculated returns by group. We identified defensives and cyclical sectors. Defensive sectors generally underperform the benchmark. They therefore have to compensate their shareholders with generous dividend yields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c742a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_info_df.groupby('sector')[['dividendYield', 'fiveYearAvgDividendYield', 'payoutRatio']].mean().sort_values(by='fiveYearAvgDividendYield', ascending=False).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_info_df[['exDividendDate', 'dividendDate']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a464613",
   "metadata": {},
   "source": [
    "### Compare valuation multiples across sectors\n",
    "\n",
    "Examine price-to-earnings and price-to-book ratios by sector to identify relatively expensive or cheap sectors.\n",
    "\n",
    "How it works:\n",
    "1. Group by sector: Aggregate stocks by GICS sector classification\n",
    "2. Calculate medians: Use median (not mean) for trailingPE, forwardPE, and priceToBook to avoid outlier distortion\n",
    "3. Sort by trailing PE: Rank sectors from most to least expensive based on historical earnings\n",
    "\n",
    "Interpretation: Technology have sported gravity defying P/E and still lead the charge in the current bull market. Forward looking P/E look reasonable while trailing P/E are rich. This means earnings momentum and growth prospects are strong. The moment earnings momentum slows down, valuations will compress and those stocks will have a rude encounter with Newtonian physics.\n",
    "High PBR indicates high leveraged balance sheets compared to valuations. As soon as the rent of money goes up, fragile sheets crumble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_info_df.groupby('sector')[['trailingPE', 'forwardPE', 'priceToBook']].median().sort_values(by='trailingPE', ascending=False).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201cab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25 ; start= '2015-01-01' ; end = None\n",
    "\n",
    "px_df = batch_px_df(tickers,batch_size, start, end,show_batch = False)\n",
    "px_df.columns.names = ['ticker']\n",
    "px_df = px_df.round(2) \n",
    "px_df_rel = round(px_df.divide(px_df[bm_ticker], axis=0).mul(px_df.iloc[0,list(px_df.columns).index(bm_ticker)]),1)\n",
    "\n",
    "px_df_rel = px_df_rel.round(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d2749",
   "metadata": {},
   "source": [
    "### Download historical price data and calculate relative performance\n",
    "\n",
    "Batch download daily close prices for all S&P 500 stocks and compute relative performance vs. benchmark.\n",
    "\n",
    "How it works:\n",
    "1. Set parameters: Define batch size (25 tickers/batch), start date (2015-01-01), and benchmark (^GSPC)\n",
    "2. Batch download: Use batch_px_df() to download all tickers in chunks to avoid API rate limits\n",
    "3. Clean data: Round prices to 2 decimals and remove timezone info\n",
    "4. Calculate relative prices: Divide each stock by benchmark and normalize to benchmark's starting value\n",
    "5. Result: Two DataFrames - px_df (absolute prices) and px_df_rel (performance relative to S&P 500)\n",
    "\n",
    "Result: Time series of daily prices where px_df_rel shows which stocks outperformed (>100) or underperformed (<100) vs. the benchmark since 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03771d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(df,bm_col):\n",
    "    log_returns = np.log(df/df.shift(1))\n",
    "    var = log_returns[bm_col].var()\n",
    "    cov = log_returns.cov()\n",
    "    beta = round(cov.loc[:,bm_col] / var,2)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa9b86",
   "metadata": {},
   "source": [
    "### Define beta calculation function\n",
    "\n",
    "Calculate stock beta (systematic risk) relative to benchmark using log returns covariance method.\n",
    "\n",
    "How it works:\n",
    "1. Compute log returns: Calculate log(price_t / price_t-1) for all stocks and benchmark\n",
    "2. Calculate variance: Compute variance of benchmark returns\n",
    "3. Compute covariance: Build covariance matrix between all stocks and benchmark\n",
    "4. Calculate beta: Divide each stock's covariance with benchmark by benchmark variance\n",
    "5. Round results: Return betas rounded to 2 decimals\n",
    "\n",
    "Result: Beta values where β>1 indicates higher volatility than market, β<1 means lower volatility, and β≈1 moves with market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6666e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_end_list = px_df.resample('ME', closed='left', label='right').last().index.to_list()\n",
    "\n",
    "window = 12 * 3\n",
    "beta_df = pd.DataFrame( )\n",
    "bm_col = '^GSPC'\n",
    "for t,dt in enumerate(month_end_list):\n",
    "    beta_monthly_df =  beta(px_df[dt:month_end_list[t + window]],bm_col)\n",
    "    beta_df = pd.concat([beta_df,beta_monthly_df],axis =1)\n",
    "    beta_df = beta_df.rename(columns={bm_col: month_end_list[t + window]})\n",
    "    if month_end_list[t + window] == month_end_list[-1]:\n",
    "        print(t,dt,month_end_list[t+window])\n",
    "        break\n",
    "\n",
    "beta_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02dc67",
   "metadata": {},
   "source": [
    "### Calculate rolling 36-month beta for all stocks\n",
    "\n",
    "Compute monthly updated betas using a 3-year rolling window to track how systematic risk evolves over time.\n",
    "\n",
    "How it works:\n",
    "1. Create month-end list: Resample price data to month-end dates for period boundaries\n",
    "2. Set rolling window: Use 36 months (12 × 3) for beta stability while capturing regime changes\n",
    "3. Loop through months: For each month, calculate beta using data from that month through +36 months ahead\n",
    "4. Build beta DataFrame: Each column represents betas as of a specific month-end date\n",
    "5. Stop at data end: Break loop when reaching the last available month\n",
    "\n",
    "Result: DataFrame with betas for each stock (rows) calculated at each month-end (columns), showing how risk profiles change across market cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP500_beta = pd.DataFrame()\n",
    "df_SP500_beta = pd.concat([sp500_info_df['sector'], beta_df], axis=1)\n",
    "\n",
    "sector_beta_pivot = pd.pivot_table(df_SP500_beta, values=list(beta_df.columns), \n",
    "                                   index= ['sector'], aggfunc=\"mean\").T\n",
    "sector_beta_pivot.plot(figsize=(15,4),title = 'Beta by Sector', grid=True)\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607786bd",
   "metadata": {},
   "source": [
    "### Visualize beta trends by sector over time\n",
    "\n",
    "Aggregate individual stock betas to sector level and plot time series to identify which sectors become more/less risky.\n",
    "\n",
    "How it works:\n",
    "1. Merge data: Combine sector labels from sp500_info_df with beta time series\n",
    "2. Pivot table: Calculate mean beta per sector for each month using pd.pivot_table()\n",
    "3. Transpose: Flip table so months are rows (x-axis) and sectors are columns (series)\n",
    "4. Plot: Generate time series chart showing beta evolution by sector\n",
    "\n",
    "Result: Line chart revealing which sectors (e.g., Technology, Energy) exhibit higher systematic risk and how sector betas shift during bull/bear markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_log_returns = np.log(px_df/px_df.shift(1))\n",
    "daily_rel_log_returns = daily_log_returns.sub(daily_log_returns[bm_col],axis=0)\n",
    "cum_returns_abs = daily_log_returns.cumsum().apply(np.exp)-1\n",
    "cum_returns_rel = daily_rel_log_returns.cumsum().apply(np.exp)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4a736",
   "metadata": {},
   "source": [
    "### Calculate absolute and relative returns for strategy analysis\n",
    "\n",
    "Compute daily log returns in both absolute and benchmark-relative terms for performance attribution.\n",
    "\n",
    "How it works:\n",
    "1. Daily log returns: Calculate log(price_t / price_t-1) for all stocks\n",
    "2. Relative log returns: Subtract benchmark returns from each stock's returns (excess returns)\n",
    "3. Cumulative absolute: Apply exp(cumsum(log_returns)) - 1 to get total returns\n",
    "4. Cumulative relative: Apply exp(cumsum(relative_log_returns)) - 1 to get benchmark-adjusted returns\n",
    "\n",
    "Result: Four DataFrames enabling analysis of both absolute performance and alpha generation (outperformance vs. S&P 500)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26373f0",
   "metadata": {},
   "source": [
    "### Calculate and visualize sector performance metrics\n",
    "\n",
    "**Objective:** Compare sector-level absolute and relative returns to identify which sectors outperform or underperform the S&P 500 benchmark.\n",
    "\n",
    "**Steps:**\n",
    "1. Merge sector labels with cumulative return time series to organize returns by sector classification\n",
    "2. Group returns by sector and calculate mean performance across all constituent stocks within each sector\n",
    "3. Plot both absolute returns (total gains) and relative returns (excess over benchmark) for sector comparison\n",
    "\n",
    "**Result:** Two visualizations showing sector performance trends—absolute chart reveals total returns while relative chart highlights which sectors generated alpha (outperformance) versus the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_abs_returns = pd.concat([sp500_info_df['sector'],cum_returns_abs.T],axis=1)\n",
    "sector_avg_abs_returns = sector_abs_returns.groupby('sector').mean().T\n",
    "sector_avg_abs_returns.plot(figsize=(15,4), title='Sector Average Absolute Returns vs S&P 500', grid=True)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "sector_rel_returns = pd.concat([sp500_info_df['sector'],cum_returns_rel.T],axis=1)\n",
    "sector_avg_rel_returns = sector_rel_returns.groupby('sector').mean().T\n",
    "sector_avg_rel_returns.plot(figsize=(15,4), title='Sector Average Relative Returns vs S&P 500', grid=True)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a3a13",
   "metadata": {},
   "source": [
    "### Compare high-beta vs. low-beta sector performance\n",
    "\n",
    "Analyze performance difference between cyclical high-beta sectors (Technology, Consumer Cyclical) and defensive low-beta sectors (Utilities, Consumer Defensive).\n",
    "\n",
    "How it works:\n",
    "1. Define sector groups: Select high-beta sectors (Technology, Consumer Cyclical) and low-beta sectors (Utilities, Consumer Defensive)\n",
    "2. Filter returns: Extract returns for stocks in each sector group from sector_rel_returns and sector_abs_returns\n",
    "3. Calculate averages: Compute mean returns across all stocks in high-beta and low-beta groups\n",
    "4. Build comparison DataFrame: Combine benchmark, high/low beta averages, and their spread (high minus low)\n",
    "5. Plot relative returns: Chart showing relative performance vs. S&P 500 for both groups and their difference\n",
    "6. Plot absolute returns: Chart showing total returns including benchmark for comparison\n",
    "\n",
    "Result: Two charts demonstrating beta factor premium—high-beta sectors outperform in bull markets but underperform in bear markets, while low-beta sectors provide defensive characteristics. The spread line shows the cyclical nature of the beta factor.\n",
    "\n",
    "Note: The absolute chart shows the returns without adjustment for the benchmark returns. This is why it looks so much better. The relative chart shows the excess returns over the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_beta_sectors = ['Consumer Cyclical', 'Technology']\n",
    "low_beta_sectors = ['Consumer Defensive', 'Utilities']\n",
    "\n",
    "high_beta_rel_returns = sector_rel_returns.loc[sector_rel_returns['sector'].isin(high_beta_sectors)].drop(columns=['sector'])\n",
    "low_beta_rel_returns = sector_rel_returns.loc[sector_rel_returns['sector'].isin(low_beta_sectors)].drop(columns=['sector'])\n",
    "\n",
    "high_beta_abs_returns = sector_abs_returns.loc[sector_abs_returns['sector'].isin(high_beta_sectors)].drop(columns=['sector'])\n",
    "low_beta_abs_returns = sector_abs_returns.loc[sector_abs_returns['sector'].isin(low_beta_sectors)].drop(columns=['sector'])\n",
    "\n",
    "beta_cum_returns = pd.DataFrame()\n",
    "beta_cum_returns[bm_col]= cum_returns_abs[bm_col].copy()\n",
    "beta_cum_returns['high_beta_rel_avg'] = high_beta_rel_returns.mean(axis=0)\n",
    "beta_cum_returns['low_beta_rel_avg'] = low_beta_rel_returns.mean(axis=0)\n",
    "beta_cum_returns['high_low_beta_rel_avg'] = beta_cum_returns['high_beta_rel_avg'].sub(\n",
    "    beta_cum_returns['low_beta_rel_avg'] )\n",
    "\n",
    "beta_cum_returns['high_beta_abs_avg'] = high_beta_abs_returns.mean(axis=0)\n",
    "beta_cum_returns['low_beta_abs_avg'] = low_beta_abs_returns.mean(axis=0)\n",
    "beta_cum_returns['high_low_beta_abs_avg'] = beta_cum_returns['high_beta_abs_avg'].sub(\n",
    "    beta_cum_returns['low_beta_abs_avg'] )\n",
    "\n",
    "beta_cum_returns[[bm_col,'high_beta_rel_avg','low_beta_rel_avg', 'high_low_beta_rel_avg']].plot(\n",
    "    grid = True, figsize= (15,4), title = 'High Low Beta Sectors Relative Returns')\n",
    "\n",
    "beta_cum_returns[[bm_col,'high_beta_abs_avg','low_beta_abs_avg', 'high_low_beta_abs_avg']].plot(\n",
    "    grid = True, figsize= (15,4), title = 'High Low Beta Sectors Absolute Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a57bb",
   "metadata": {},
   "source": [
    "### Combine sector information with cumulative returns\n",
    "\n",
    "Merge sector labels with cumulative return matrices to enable sector-level performance analysis.\n",
    "\n",
    "How it works:\n",
    "1. Transpose returns: Flip cum_returns DataFrames so tickers are rows (matching sp500_info_df index)\n",
    "2. Concatenate sector: Add 'sector' column from sp500_info_df to returns data\n",
    "3. Create both versions: Build sector_rel_returns (benchmark-adjusted) and sector_abs_returns (total returns)\n",
    "\n",
    "Result: DataFrames indexed by ticker with sector label + all daily cumulative returns, ready for groupby operations to analyze sector performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35652a",
   "metadata": {},
   "source": [
    "### Backtest high-beta vs. low-beta stock strategy\n",
    "\n",
    "This code backtests a daily-rebalanced beta factor strategy tracking high-beta vs. low-beta stock portfolios.\n",
    "\n",
    "Steps:\n",
    "1. Each month, select the 5 highest-beta and 5 lowest-beta stocks based on previous month's rankings\n",
    "2. Calculate mean daily relative log returns across the 5 stocks in each portfolio during the holding period\n",
    "3. Concatenate all daily returns across holding periods into continuous time series\n",
    "4. Compute the long-short spread (top minus bottom), add benchmark returns, and apply cumulative sum\n",
    "5. Plot cumulative performance showing how each strategy performs over time vs. the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d479363",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = 5\n",
    "top_beta_returns_list = []\n",
    "bottom_beta_returns_list = []\n",
    "beta_returns_df = pd.DataFrame()\n",
    "\n",
    "for t in range(1,len(beta_df.columns)):\n",
    "    period_start = beta_df.columns[t-1]\n",
    "    period_end = beta_df.columns[t]\n",
    "\n",
    "    top_beta = beta_df[period_start].nlargest(n = pos_count).index\n",
    "    bottom_beta = beta_df[period_start].nsmallest(n = pos_count).index\n",
    "    top_beta_returns = daily_rel_log_returns.loc[period_start:period_end, top_beta].iloc[1:].mean(axis=1)#.sum()\n",
    "    bottom_beta_returns = daily_rel_log_returns.loc[period_start:period_end, bottom_beta].iloc[1:].mean(axis=1)#.sum()\n",
    "\n",
    "    top_beta_returns_list.append(top_beta_returns)\n",
    "    bottom_beta_returns_list.append(bottom_beta_returns)\n",
    "\n",
    "top_beta_returns_df = pd.concat(top_beta_returns_list)\n",
    "bottom_beta_returns_df = pd.concat(bottom_beta_returns_list)\n",
    "\n",
    "beta_returns_df[f'top_{pos_count}_beta']  = top_beta_returns_df\n",
    "beta_returns_df[f'bottom_{pos_count}_beta']  = bottom_beta_returns_df\n",
    "beta_returns_df[f'net_{pos_count}_beta']  = beta_returns_df[f'top_{pos_count}_beta'].sub(beta_returns_df[f'bottom_{pos_count}_beta'])\n",
    "beta_returns_df[bm_col] = daily_log_returns[bm_col]\n",
    "beta_returns_df = beta_returns_df.cumsum()\n",
    "beta_returns_df.plot(figsize=(15,4), grid = True, title=f'Top vs Bottom Beta {pos_count} Stocks Cumulative Returns'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b840a76",
   "metadata": {},
   "source": [
    "### Backtest beta momentum strategy (rising vs. falling betas)\n",
    "\n",
    "This code tests a strategy based on beta changes rather than absolute beta levels, targeting stocks with rising or falling systematic risk.\n",
    "\n",
    "Steps:\n",
    "1. Calculate month-over-month percentage change in beta values for all stocks to identify momentum in risk profiles\n",
    "2. Each month, select the 5 stocks with the largest beta increases (rising risk) and 5 with the largest beta decreases (falling risk)\n",
    "3. Calculate mean daily relative log returns for each portfolio during the holding period\n",
    "4. Concatenate daily returns into continuous time series, compute long-short spread (falling minus rising), and add benchmark\n",
    "5. Plot cumulative performance to determine if betting against beta momentum (short rising, long falling) generates alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73781c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_diff = beta_df.pct_change(axis=1, fill_method = None).dropna(axis=1, how = 'all')\n",
    "top_beta_diff_returns_list = []\n",
    "bottom_beta_diff_returns_list = []\n",
    "beta_returns_diff = pd.DataFrame()\n",
    "\n",
    "for t in range(1,len(beta_diff.columns)):\n",
    "    period_start = beta_diff.columns[t-1]\n",
    "    period_end = beta_diff.columns[t]\n",
    "\n",
    "    top_beta = beta_diff[period_start].nlargest(n = pos_count).index\n",
    "    bottom_beta = beta_diff[period_start].nsmallest(n = pos_count).index\n",
    "    top_beta_diff_returns = daily_rel_log_returns.loc[period_start:period_end, top_beta].iloc[1:].mean(axis=1)#.sum()\n",
    "    bottom_beta_diff_returns = daily_rel_log_returns.loc[period_start:period_end, bottom_beta].iloc[1:].mean(axis=1)#.sum()\n",
    "\n",
    "    top_beta_diff_returns_list.append(top_beta_diff_returns)\n",
    "    bottom_beta_diff_returns_list.append(bottom_beta_diff_returns)\n",
    "top_beta_diff_returns_df = pd.concat(top_beta_diff_returns_list)\n",
    "bottom_beta_diff_returns_df = pd.concat(bottom_beta_diff_returns_list)\n",
    "\n",
    "beta_returns_diff[f'rising_{pos_count}_beta']  = top_beta_diff_returns_df\n",
    "beta_returns_diff[f'falling_{pos_count}_beta']  = bottom_beta_diff_returns_df\n",
    "beta_returns_diff[f'net_{pos_count}_beta']  = beta_returns_diff[f'falling_{pos_count}_beta'].sub(beta_returns_diff[f'rising_{pos_count}_beta'])\n",
    "beta_returns_diff[bm_col] = daily_log_returns[bm_col]\n",
    "beta_returns_diff = beta_returns_diff.cumsum()\n",
    "beta_returns_diff.plot(figsize=(15,4), grid = True, title=f'{pos_count} Fallling V. Rising Betas Cumulative Returns'  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
